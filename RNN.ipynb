{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/qEciKRn9ysjnQiR0l1qF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanStart/CNN-Model/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XzRzKGIuLDF4"
      },
      "outputs": [],
      "source": [
        "#Importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN:\n",
        "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
        "      # hyper parameters\n",
        "      self.hidden_size = hidden_size\n",
        "      self.vocab_size = vocab_size\n",
        "      self.seq_length = seq_length\n",
        "      self.learning_rate = learning_rate\n",
        "      # model parameters\n",
        "      self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
        "      self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
        "      self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
        "      self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
        "      self.c = np.zeros((vocab_size, 1)) # bias for output\n"
      ],
      "metadata": {
        "id": "N9SqIjWTLYC8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, inputs, hprev):\n",
        "  xs, hs, os, ycap = {}, {}, {}, {}\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  for t in range(len(inputs)):\n",
        "    xs[t] = zero_init(self.vocab_size, 1)\n",
        "    xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
        "    hs[t] = np.tanh(np.dot(self.U, xs[t]) + np.dot(self.W, hs[t-1]) + self.b) # hidden state\n",
        "    os[t] = np.dot(self.V, hs[t]) + self.c #unnormalized log probs for next char\n",
        "    ycap[t] = self.softmax(os[t]) # probs for next char\n",
        "  return xs, hs, ycap"
      ],
      "metadata": {
        "id": "ZhgIdCQdP5vC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(self, x):\n",
        "  p = np.exp(x - np.max(x))\n",
        "  return p / np.sum(p)"
      ],
      "metadata": {
        "id": "Abc8fulxSDwb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(self, ps, targets):\n",
        "  \"\"\" loss for a sequence \"\"\"\n",
        "  # calculate cross-entropy loss\n",
        "  return sum(-np.log(ps[t][targets[t], 0]) for t in range(self.seq_length))"
      ],
      "metadata": {
        "id": "AVYRCdPrS9r5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(self, xs, hs, ycap, targets):\n",
        "  # backward pass: compute gradients going backwrds\n",
        "  dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self, W), np.zeros_like(self.V)\n",
        "  db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(range(self.seq_lenght)):\n",
        "    # start with output\n",
        "    dy = np.copy(ycap[t])\n",
        "    # gradient through oftmax\n",
        "    dy[targets[t]] -= 1\n",
        "    # dv and dc\n",
        "    dV += np.dot(dy, hs[t].T)\n",
        "    dc += dc\n",
        "    # dh has two components, gradient flowing from output and from next cell\n",
        "    dh = np.dot(self.V.T, dy) + dhnext #backprop into h\n",
        "    # dhrec is the reccuring component seen in most of the calculations\n",
        "    dhrec = (1 - hs[t] * hs[t] * dh)  # backprop through tanh non-linearity\n",
        "    db += dhrec\n",
        "    # dU and dW\n",
        "    dU += np.dot(dhrec, xs[t].T)\n",
        "    dW += np.dot(dhrec, hs[t - 1].T)\n",
        "    # pass the gradients from next cell for next iteration\n",
        "    dhnext = np.dot(self.W.T, dhrec)\n",
        "  #To mitigate gradient explosion, clip the gradients.\n",
        "  for dparam in [dU, dW, dV, db, dc]:\n",
        "    np.clip(dparam, -5, 5, out=dparam)\n",
        "  return dU, dW, dV, db, dc"
      ],
      "metadata": {
        "id": "3Dv9hnY7VvQO"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}